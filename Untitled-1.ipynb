{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!pip install -U scikit_learn pandas 'numpy<=1.23.0' seaborn matplotlib"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-05T01:21:57.307004Z","iopub.status.busy":"2023-07-05T01:21:57.306512Z","iopub.status.idle":"2023-07-05T01:22:01.514955Z","shell.execute_reply":"2023-07-05T01:22:01.513420Z","shell.execute_reply.started":"2023-07-05T01:21:57.306960Z"},"id":"mmwBzpblmnjH","trusted":true},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","import pathlib as pl\n","from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n","from sklearn.model_selection import GridSearchCV, train_test_split\n","from sklearn.calibration import CalibratedClassifierCV, CalibrationDisplay\n","from sklearn.metrics import f1_score\n","\n","from sklearn.experimental import enable_iterative_imputer\n","from sklearn.impute import IterativeImputer"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"6sHFpppPmnjJ"},"source":["# Load the Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-05T01:37:52.376635Z","iopub.status.busy":"2023-07-05T01:37:52.376180Z","iopub.status.idle":"2023-07-05T01:37:52.428129Z","shell.execute_reply":"2023-07-05T01:37:52.426909Z","shell.execute_reply.started":"2023-07-05T01:37:52.376594Z"},"id":"c1P3Y3a7mnjL","trusted":true},"outputs":[],"source":["# Load a dataset into a Pandas Dataframe\n","data_path = pl.Path('/kaggle/input/spaceship-titanic/')\n","if not data_path.exists():\n","    data_path = pl.Path('data')\n","# df = pd.read_csv('/kaggle/input/spaceship-titanic/train.csv')\n","df = pd.read_csv(data_path/'train.csv')\n","print(\"Full train dataset shape is {}\".format(df.shape))\n","label='Transported'\n","df.head(5)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Before starting, there are some columns that can be further broken down into more data, which will make our analysis more complete.\n","\n","After we impute missing data in the original columns, we'll have to generate these surrogate columns again."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def extract_l1_features(df):\n","    df[[\"PassengerGroup\", \"PassengerGroupId\"]] = df[\"PassengerId\"].str.split('_', expand = True)\n","    No_People_In_PassengerGroup = df.groupby('PassengerGroup').aggregate({'PassengerId': 'size'}).reset_index()\n","    No_People_In_PassengerGroup = No_People_In_PassengerGroup.rename(columns = {\"PassengerId\": \"NoInPassengerGroup\"})\n","    df = df.merge(No_People_In_PassengerGroup[[\"PassengerGroup\"]], how = 'left', on = ['PassengerGroup'])\n","\n","    # Split Cabin into Deck, Number and Side features\n","    df[[\"CabinDeck\", \"CabinNum\", \"CabinSide\"]] = df[\"Cabin\"].str.split(\"/\", expand=True)\n","\n","    # Create TotalSpendings feature\n","    df[\"TotalSpendings\"] = df[\"RoomService\"] + df[\"FoodCourt\"] + df[\"ShoppingMall\"] + df[\"Spa\"] + df[\"VRDeck\"]\n","    \n","    df[[\"FirstName\", \"FamilyName\"]] = df[\"Name\"].str.split(' ', expand = True)\n","    # Create NoRelatives feature\n","    NoRelatives = df.groupby('FamilyName')['PassengerId'].count().reset_index()\n","    NoRelatives = NoRelatives.rename(columns = {\"PassengerId\": \"NoRelatives\"})\n","    \n","    df = df.merge(NoRelatives[[\"FamilyName\", \"NoRelatives\"]], how = 'left', on = ['FamilyName'])\n","    df.drop(columns=['Cabin','Name'],inplace=True)\n","\n","    return df\n","\n","df1 = extract_l1_features(df.copy())\n","df1.head()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Helper functions to plot univariate and bivariate charts for categorical and numeric data."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-05T01:38:04.833415Z","iopub.status.busy":"2023-07-05T01:38:04.832970Z","iopub.status.idle":"2023-07-05T01:38:05.978456Z","shell.execute_reply":"2023-07-05T01:38:05.977206Z","shell.execute_reply.started":"2023-07-05T01:38:04.833377Z"},"id":"DcaGweARmnjP","trusted":true},"outputs":[],"source":["def config_axes(list_df_cols, dependent: str | None = None, n_per_row=None):\n","    if dependent is not None and dependent in list_df_cols:\n","        list_df_cols.remove(dependent)\n","\n","    if n_per_row:\n","        nrows = len(list_df_cols)\n","        ncols = n_per_row\n","    else:\n","        sqrt_n_cols = np.sqrt(len(list_df_cols))\n","        # nrows = int(np.floor(sqrt_n_cols))\n","        nrows = ncols = int(np.ceil(sqrt_n_cols))\n","\n","    figsize = (ncols * 5, nrows * 4)\n","    _, axes = plt.subplots(nrows, ncols, figsize=figsize)\n","    return axes\n","\n","\n","def plot_cat_cols(df: pd.DataFrame, dependent: str | None = None):\n","    list_df_cols = sorted(\n","        list(df.select_dtypes([\"object\", \"category\", \"bool\"]).columns)\n","    )\n","    if dependent is not None and dependent in list_df_cols:\n","        list_df_cols.remove(dependent)\n","\n","    axes = config_axes(list_df_cols, dependent).flat\n","    axes = iter(axes)\n","\n","    for col in list_df_cols:\n","        ax = next(axes)\n","        if dependent is None:\n","            sns.countplot(data=df, x=col, order=df[col].sort_values().unique(), ax=ax)\n","        else:\n","            sns.barplot(\n","                data=df,\n","                x=col,\n","                y=dependent,\n","                order=df[col].sort_values().unique(),\n","                orient=\"v\",\n","                ax=ax,\n","            )\n","        ax.bar_label(ax.containers[0])\n","\n","    plt.tight_layout()\n","\n","\n","def plot_numeric_cols(df: pd.DataFrame, dependent: str | None = None):\n","    list_df_cols = list(df.select_dtypes(np.number).columns)\n","\n","    axes = iter(config_axes(list_df_cols, dependent, 2).flat)\n","    for col in list_df_cols:\n","        if dependent is None:\n","            sns.histplot(\n","                data=df,\n","                color=\"b\",\n","                x=col,\n","                ax=next(axes),\n","            )\n","            sns.boxplot(\n","                data=df,\n","                color=\"b\",\n","                y=col,\n","                ax=next(axes),\n","            )\n","        else:\n","            sns.violinplot(\n","                data=df,\n","                x=dependent,\n","                y=col,\n","                ax=next(axes),\n","            )\n","            sns.boxplot(\n","                data=df,\n","                x=dependent,\n","                y=col,\n","                ax=next(axes),\n","            )\n","\n","    plt.tight_layout()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Univariate analysis"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Categorical variables"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# remove columns with too many values\n","plot_cat_cols(\n","    df1.drop(\n","        columns=[\"PassengerId\", \"FirstName\", \"FamilyName\", \"CabinNum\", \"PassengerGroup\"]\n","    )\n",")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Conclusions\n","\n","- Cabin decks F and G have a lot more people than the other ones, it may be valuable to see if other features explain this disparity. Maybe rich and poor people travel separately, like in the original Titanic?\n","- Cabin deck T has only 5 people, who are they?\n","- Most people are going to TRAPPIST-1e.\n","- Most people come from Earth.\n","- The dataset is balanced: roughly the same amount of transported and non-transported people in the ship.\n","- PassengerGroupId has logarithmic behavior, but this is just the nature of the data (all groups have at least one person with group ID 1 and larger groups are more rare)."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"LRO2hJlNmnjP"},"source":["## Numerical variables"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-05T01:38:06.062096Z","iopub.status.busy":"2023-07-05T01:38:06.061299Z","iopub.status.idle":"2023-07-05T01:38:07.921281Z","shell.execute_reply":"2023-07-05T01:38:07.920014Z","shell.execute_reply.started":"2023-07-05T01:38:06.062046Z"},"id":"lafxj4fkmnjQ","trusted":true},"outputs":[],"source":["plot_numeric_cols(df1)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Conclusions\n","\n","- Age is not totally normally distributed. There are more young people than old.\n","- Most people spend very little to no money. Are there lots of poor people or is there some explanation to this?"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Bivariate analysis against dependent variable"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Categorical variables"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# remove columns with too many values\n","plot_cat_cols(df1.drop(columns=[\"PassengerId\", \"FirstName\",'FamilyName','CabinNum','PassengerGroup']), dependent=label)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Conclusions\n","\n","- Cabin sides B and C have more transported people (proportionally, inside the group). \n","- Even though we have more people coming from Earth and going to TRAPPIST-1e, these are the sources of the fewest transported people.\n","- People in cryogenic sleep have been transported much more than awake people."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Numerical variables"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plot_numeric_cols(df, dependent=label)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Conclusions\n","\n","- Age has no bearing on who gets transported.\n","- For some reason, people who get transported spend **less** money on room service, Spa and VR deck."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Bivariate analysis, misc"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["There could be something going on with lots of people in cryogenic sleep being transported and people who spend less in certain activities also being transported. Let's check out the relationship between CryoSleep and the numeric variables. "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plot_numeric_cols(df, dependent='CryoSleep')"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Conclusions\n","\n","- Age has no bearing on who gets turned into a popsicle.\n","- People in cryogenic sleep spend no money **at all**. but because of that, we don't know if they are wealthy or not."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Let's also look at passenger spending by age. Maybe old people are richer and spend more, while kids spend less."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df[\"AgeCat\"] = pd.cut(\n","    df.Age,\n","    bins=[0, 4, 12, 17, 25, 34, 55, 80],\n","    labels=[\"0 - 4\", \"5 - 12\", \"13 - 17\", \"18 - 25\", \"26 - 34\", \"35 - 55\", \"56 - 80\"],\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plot_numeric_cols(df.drop(columns='Age'), dependent='AgeCat')"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Conclusions\n","\n","- Kids also do no spend any money."]},{"cell_type":"markdown","metadata":{},"source":["A little bird told me to take a look at this one..."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pd.crosstab(df['AgeCat'],df['VIP'])"]},{"cell_type":"markdown","metadata":{},"source":["## Conclusions\n","\n","- People under 18 are not VIPs."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["del df['AgeCat']"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Let's look at who comes from where and who goes where in this ship."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sns.heatmap(pd.crosstab(df1.HomePlanet,df1.Destination), annot=True, fmt=\"d\", linewidths=.5);"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Conclusions\n","\n","- Most people who embark on Earth go to TRAPPIST-1e."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## TODO\n","\n","- Visualize spending, cryo sleep, home planet and destination by deck.\n","- Analyze the compositions of cabins B/C (the ones with most transported) and F/G (the ones with the most people) against CryoSleep and spending."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Dealing with missing values\n","\n","In this section, we'll deal with the missing data in the original dataset, without the generated features. After inserting as much missing data as we can, we'll create those columns again."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df.isna().sum()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Let's use our knowledge that kids and sleepers don't spend and fill those missing values in a more informed way."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def fill_nans_by_age_and_cryosleep(df):\n","    non_spenders = (df[\"Age\"] < 13) | (df[\"CryoSleep\"] == True)\n","    df.loc[non_spenders, \"RoomService\"] = 0\n","    df.loc[non_spenders, \"FoodCourt\"] = 0\n","    df.loc[non_spenders, \"ShoppingMall\"] = 0\n","    df.loc[non_spenders, \"Spa\"] = 0\n","    df.loc[non_spenders, \"VRDeck\"] = 0\n","    df.loc[df[\"Age\"] < 18, \"VIP\"] = False\n","    return df\n","\n","df = fill_nans_by_age_and_cryosleep(df)\n","df.isna().sum()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["The rest of categorical variables will be filled with the mode."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Clever way to list categorical variables with missing values\n","list_missing_cat_columns = list(\n","    (df.select_dtypes([\"object\", \"category\", \"bool\"]).isna().sum() > 0).index\n",")\n","\n","for col in list_missing_cat_columns:\n","    df[col] = df[col].fillna(df[col].mode()[0])\n","df.isna().sum()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Rows with missing numeric values will be filled with sklearn Iterative Imputer. "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["list_missing_numeric_col = list((df.select_dtypes(np.number).isna().sum() > 0).index)\n","list_numeric_col = list(df.select_dtypes(np.number).columns)\n","df[list_missing_numeric_col] = pd.DataFrame(IterativeImputer().fit_transform(df[list_numeric_col]), columns=[list_missing_numeric_col])\n","df.isna().sum()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Clip outliers in numerical columns on the 99% quantile."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def clipping_quantile(dataframe, quantile_values = None, quantile = 0.99):\n","    df = dataframe.copy()\n","    if quantile_values is None:\n","        quantile_values = df[list_numeric_col].quantile(quantile)\n","    for num_column in list_numeric_col:\n","        num_values = df[num_column].values\n","        threshold = quantile_values[num_column]\n","        num_values = np.where(num_values > threshold, threshold, num_values)\n","        df[num_column] = num_values\n","    return df      \n","    \n","df = clipping_quantile(df, None, 0.99)\n","plot_numeric_cols(df)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Now we'll create some additional features based on our previous findings."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def extract_l2_features(df):    \n","    # Create DeckPosition feature\n","    df[\"DeckPosition\"] = df[\"CabinDeck\"].apply(lambda deck: \"Lower\" if deck in ('A', 'B', 'C', 'D') else \"Higher\" )\n","    # Create Regular feature\n","    df[\"Regular\"] = df[\"FoodCourt\"] + df[\"ShoppingMall\"] \n","    # Create Luxury feature\n","    df[\"Luxury\"] = df[\"RoomService\"] + df[\"Spa\"] + df[\"VRDeck\"]\n","    \n","    Wealthiest_Deck = df.groupby('CabinDeck').aggregate({'TotalSpendings': 'sum', 'PassengerId': 'size'}).reset_index()\n","    # Create DeckAverageSpent feature\n","    Wealthiest_Deck['DeckAverageSpent'] = Wealthiest_Deck['TotalSpendings'] / Wealthiest_Deck['PassengerId']\n","\n","    df.drop(columns=['PassengerId'],inplace=True)\n","    \n","    df = df.merge(Wealthiest_Deck[[\"CabinDeck\", \"DeckAverageSpent\"]], how = 'left', on = ['CabinDeck'])\n","    # Create FamilySizeCat feature\n","    df[\"FamilySizeCat\"] = pd.cut(df.NoRelatives, bins = [0, 2, 5, 10, 300], labels = ['0 - 2', '3 - 5', '6 - 10', '11 - 208'])\n","    \n","    return df\n","\n","df2 = df.copy()\n","df2 = extract_l1_features(df)\n","df2 = extract_l2_features(df2)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df3 = df2.copy()\n","irrelevant_columns = [\"RoomService\", \"FoodCourt\", \"ShoppingMall\", \"Spa\", \"VRDeck\", \"FirstName\", \"FamilyName\", \"PassengerGroup\"]\n","df3.drop(columns=irrelevant_columns,inplace=True)\n","df3.info()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Feature encoding\n","\n","Depending on the algorithm and library used, some features need to be transformed further for compatibility, e.g.:\n","- tfdf does not work with Boolean values, but works with Categorical columns and treats string columns as Categorical.\n","- sklearn does not work with Categorical or string columns.\n","\n","It is usually a good idea to convert:\n","- Boolean values into 0/1 integers\n","- Categorical values into one-hot representations\n","- Ordinal categorical values into ordinal representations"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Preprocessing for sklearn decision trees\n","# Categorical encoding, drop redundant columns\n","df3 = pd.get_dummies(\n","    df3,\n","    columns=[\"HomePlanet\", \"CryoSleep\", \"Destination\", \"VIP\", \"CabinSide\"],\n","    dtype=int, drop_first=True,\n",")\n","# # # Ordinal Encoding\n","for col in [\"CabinDeck\", \"DeckPosition\", \"FamilySizeCat\"]:\n","    df3[col], _ = df3[col].factorize()\n","\n","df3.head()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Train/Val split\n","\n","Because trees can be evaluated using OOB data, using a train/val split is not necessary for evaluating the model. But it shall be done too."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["X = df3.copy()\n","X.drop('Transported', axis=1, inplace=True)\n","y = df3['Transported'].copy().astype(int)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42,stratify=y)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"sy81fpfxmnjY"},"source":["# Configure the model\n","\n","Let's do a grid search over a random forest classifier. The grid search will use straified cross-validation and will use the F1-score as selection metric."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-05T01:58:53.776007Z","iopub.status.busy":"2023-07-05T01:58:53.775143Z","iopub.status.idle":"2023-07-05T01:58:53.800288Z","shell.execute_reply":"2023-07-05T01:58:53.798790Z","shell.execute_reply.started":"2023-07-05T01:58:53.775946Z"},"id":"j7-gFVDNmnjZ","trusted":true},"outputs":[],"source":["# Create a Random Search tuner\n","parameters = {\"n_estimators\": (10, 20, 50, 100), \"max_depth\": (10, 20, 50, 100, None)}\n","grid_search = GridSearchCV(\n","    RandomForestClassifier(criterion=\"log_loss\",oob_score=f1_score), parameters, n_jobs=-1,scoring=f1_score,cv=5\n",")"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"wEQXtv4MmnjZ"},"source":["# Train the model\n","\n","We will train the model using a one-liner."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-05T01:58:55.989406Z","iopub.status.busy":"2023-07-05T01:58:55.988581Z"},"id":"QcL5KRyLmnja","trusted":true},"outputs":[],"source":["grid_search.fit(X_train, y_train)"]},{"cell_type":"markdown","metadata":{},"source":["Let's check the parameters of the best model found by the grid search."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["grid_search.best_params_\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"1q5zxzuLmnjb"},"source":["# Evaluate the model on the Out of bag (OOB) data and the validation dataset\n","\n","Before training the dataset we have manually separated 20% of the dataset for validation named as `valid_ds`.\n","\n","We can also use Out of bag (OOB) score to validate our random forest estimator.\n","To train a Random Forest Model, a set of random samples from training set are choosen by the algorithm and the rest of the samples are used to finetune the model.The subset of data that is not chosen is known as Out of bag data (OOB).\n","OOB score is computed on the OOB data.\n","\n","Read more about OOB data [here](https://developers.google.com/machine-learning/decision-forests/out-of-bag).\n","\n","The `oob_score_`attribute shows the evaluated metric on the out-of-bag dataset according to the number of trees in the model.\n","\n","Note: Larger values are better for this hyperparameter."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","rf = grid_search.best_estimator_\n","rf.oob_score_"]},{"cell_type":"markdown","metadata":{},"source":["Gini importances"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.barh(rf.feature_names_in_, rf.feature_importances_)\n","# plt.xticks(rotation=90)\n","plt.xlabel(\"Importances\")\n","plt.ylabel(\"Features\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["y_pred=rf.predict(X_test)\n","f1_score(y_test, y_pred)"]},{"cell_type":"markdown","metadata":{},"source":["# Model calibration\n","\n","This step is mostly useless, as it does not change the output of the model. But I was learning about it and decided to include it here."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["CalibrationDisplay.from_estimator(rf, X, y)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["calib_rf = CalibratedClassifierCV(rf,cv=3)\n","calib_rf.fit(X, y)\n","CalibrationDisplay.from_estimator(calib_rf, X, y);"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"DAtAR0vkmnje"},"source":["# Submission"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def process_features(df: pd.DataFrame)->pd.DataFrame:\n","    df=df.copy()\n","    df = extract_l1_features(df)\n","    df = fill_nans_by_age_and_cryosleep(df)\n","    df = extract_l2_features(df)\n","        # Clever way to list categorical variables with missing values\n","    list_missing_cat_columns = list(\n","        (df.select_dtypes([\"object\", \"category\", \"bool\"]).isna().sum() > 0).index\n","    )\n","    for col in list_missing_cat_columns:\n","        df[col] = df[col].fillna(df[col].mode()[0])\n","\n","    list_missing_numeric_col = list((df.select_dtypes(np.number).isna().sum() > 0).index)\n","    list_numeric_col = list(df.select_dtypes(np.number).columns)\n","    df[list_missing_numeric_col] = pd.DataFrame(IterativeImputer().fit_transform(df[list_numeric_col]), columns=[list_missing_numeric_col])\n","    df = clipping_quantile(df, None, 0.99)\n","    irrelevant_columns = [\"RoomService\", \"FoodCourt\", \"ShoppingMall\", \"Spa\", \"VRDeck\", \"FirstName\", \"FamilyName\", \"PassengerGroup\"]\n","    df.drop(columns=irrelevant_columns,inplace=True)\\\n","\n","    # Preprocessing for sklearn decision trees\n","    # Categorical Encoding\n","    df = pd.get_dummies(\n","        df,\n","        columns=[\"HomePlanet\", \"CryoSleep\", \"Destination\", \"VIP\", \"CabinSide\"],\n","        dtype=int, drop_first=True,\n","    )\n","    # # # Ordinal Encoding\n","    for col in [\"CabinDeck\", \"DeckPosition\", \"FamilySizeCat\"]:\n","        df[col], _ = df[col].factorize()\n","        \n","    return df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KHBRzBD9mnjf","trusted":true},"outputs":[],"source":["# Load the test dataset\n","test_df = pd.read_csv(data_path / 'test.csv')\n","submission_id = test_df.PassengerId\n","test_df = process_features(test_df)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Get the predictions for testdata\n","predictions = rf.predict(test_df)\n","n_predictions = (predictions > 0.5).astype(bool)\n","output = pd.DataFrame({'PassengerId': submission_id,\n","                       'Transported': n_predictions.squeeze()})\n","output.to_csv('submission.csv', index=False)\n","output.head()"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":4}
